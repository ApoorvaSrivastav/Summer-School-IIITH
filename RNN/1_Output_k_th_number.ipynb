{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "1-Output-k-th-number.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThrupthiAnn/SummerSchool2021_HandsOn_Aug7/blob/main/RNN/1_Output_k_th_number.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6i_kV3sa4As"
      },
      "source": [
        "## Memorize Kth number in a sequence of numbers ##\n",
        "\n",
        "This being the first problem in the RNN lab, we would start with a toy example, which is quick to train and easy to understand. What we got to do is to train a recurrent network to always remember the number appearing in a particular position in any sequence of integers.\n",
        "\n",
        "Below examples show few cases where the number to be remembered is at position 2\n",
        "\n",
        "`6,9,8,1,8,3,4  -- > 9`<br>\n",
        "`1,2,3,4 --> 2`<br>\n",
        "`9,7,2,3,4,5,6,1,1,1 -- > 7`<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1e1g5VFa4A0"
      },
      "source": [
        "### Overview ###\n",
        "\n",
        "\n",
        "This exercise is planned as below:\n",
        "\n",
        "       1. First, we we discuss how the integers are represented \n",
        "       2. Then we see how the input and outputs to the network look like\n",
        "       3. We set up the RNN network and define how the forward pass mus be done\n",
        "       4. We explore the memory retention abilities of a vanilla RNN vs an LSTM or GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm5gj_Sua4A3"
      },
      "source": [
        "# coding: utf-8\n",
        "# =============================================================================\n",
        "# Make an RNN output kth integer in a sequence of N integers ( N > k)\n",
        "# Sequences could be of any length\n",
        "\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from time import sleep\n",
        "import random\n",
        "import sys\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "random.seed( 10 ) # set the random seed (for reproducibility)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRYQZQRJa4A6"
      },
      "source": [
        "## Preparing the Training Data ##\n",
        "\n",
        "###   Random sequences of integers in [0-9] range are generated as training data###\n",
        "The function `getSample` below takes a sequence length `seqL` as input and returns a training sample to be fed to the RNN.\n",
        "\n",
        "\n",
        "For a given length `seqL`, a training sample is a 2-tuple of input -$x$ and output -$y$ where\n",
        "\n",
        "* `input` is a tensor of size [`seqLx10`]:<br>\n",
        "\t* Second dimension is 10 since each integer is represented as a one-hot vector of dimension=10 (since we in this case consider only sequence of integers in [0-9]\n",
        "\n",
        "\n",
        "* `output` is just a single number . Its the number at the Kth position which need to be remembered. So $y$ is a tensor with just one element"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4h8sdz2a4A8"
      },
      "source": [
        "def getSample(seqL,k,testFlag=False):\n",
        "    #returns a random sequence of integers of length = seqL\n",
        "    kthInt=0\n",
        "    x =  torch.zeros(seqL,10)\n",
        "    for i in range(0,seqL):\n",
        "        randomIntegerNumber = random.randint(0,9)\n",
        "        if i==k-1:\n",
        "            kthInt=randomIntegerNumber\n",
        "        if testFlag:\n",
        "            sys.stdout.write(str(randomIntegerNumber) + ' ')\n",
        "        x[i,randomIntegerNumber-1] = 1\n",
        "\n",
        "    if testFlag:\n",
        "            sys.stdout.write('--> ' + str(kthInt) + '\\n')\n",
        "    x=x.unsqueeze(1) #extra dimension for Batch\n",
        "    y=torch.tensor([kthInt]) #target is the number at kth position in the sequence \n",
        "\n",
        "    return x,y\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqQdHLrSa4A-"
      },
      "source": [
        "### Model Input ###\n",
        "As noted above, the inputs are `seqLx10` dimensional tensors, where first row corresponds to the first number in the sequence and last row is for the last number in the sequence. And within a row, the values are zero except for column where $columnIndex == k-1$, where $k$ the position of the number we need to memorize\n",
        "The `input` rows are fed  one-by-one, starting from the top, proceeding all the way to the last row.<br>\n",
        "Hence at each   `timestep` the input to the network is of a dimension=10<br>\n",
        "__Note__: This is not shown explicitly in the provided code, but is done internally. ie, RNN unrolling for a sequence is handled internally, we need not explicity loop through the timesteps.\n",
        "\n",
        "\n",
        "\n",
        "### Model Output  ###\n",
        "At the last timestep, the model should output the number at position $k$. For this purpose we have an output layer of 10 neurons. Each corresponding to integers 0-9. So if the $k^{th}$ integer is say 7, then the 7th neuron should fire the most. Or in other words output of this node should have the maximum value . \n",
        "\n",
        "\n",
        "## Training Loss ##\n",
        "We use the `CrossEntropy Loss'  to compare the predicted value $y_t$, and the target $\\tilde{y_t}$:\n",
        "\n",
        "\n",
        "\n",
        "__Note__: The loss is computed only at the last time step.\n",
        "\n",
        "The image below shows a schematic of the \"unrolled\" RNN in our case. For the ease of visualizing we show only two hidden units. In practice it could be more:\n",
        "\n",
        "In the picture $\\tilde{y_t}$ is [1,0,0,0,0,0,0,0,0,0] since the required number is 0\n",
        "\n",
        "<img src=\"https://github.com/ThrupthiAnn/SummerSchool2021_HandsOn_Aug7/blob/main/RNN/data/KthNo_architecture.jpg?raw=1\" />\n",
        "\n",
        "\n",
        "## Model Implementation ##\n",
        "The following class `Memorize` implements the above RNN. We only give the forward-pass implementation.\n",
        "The backward pass is calcuated automatically by PyTorch's auto-grad.\n",
        "\n",
        "The only parameters passed to the model is the size (dimensionality) of the hidden state.\n",
        "Bigger state, means higher capacity.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u86-jIDha4BA"
      },
      "source": [
        "class Memorize (nn.Module):\n",
        "    def __init__(self,stateDim):\n",
        "        super(Memorize, self).__init__()\n",
        "        self.stateDim = stateDim\n",
        "        self.inputDim = 10  # integer is represented as 1 hot vector of dimension=10\n",
        "        self.outputDim = 10  # 10 nodes for 10 classes\n",
        "        # currently the model uses the 'LSTM' cell. You could try\n",
        "        # others like: tanh, GRU. See: https://github.com/pytorch/examples/blob/master/word_language_model/model.py#L11\n",
        "        self.lstm = nn.LSTM(self.inputDim, self.stateDim )\n",
        "        self.outputLayer = nn.Linear(self.stateDim, self.outputDim)\n",
        "        self.softmax = nn.Softmax()\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        X: [L,B,inputDim(=10)] dimensional input tensor\n",
        "            L: Sequence length\n",
        "            B: is the \"batch\" dimension. As we are training on \n",
        "               single examples, B = 1 for us.\n",
        "        \"\"\"\n",
        "        lstmOut,_ = self.lstm(x)\n",
        "        L,B,D  = lstmOut.size(0),lstmOut.size(1),lstmOut.size(2) # L is seq len, B is batch size and D is feature dimension\n",
        "        #lstmOut holds the outputs at all timesteps but we require  only the output at last time step (L-1)\n",
        "        lstmOut_lastTimeStep = lstmOut[L-1,0,:]\n",
        "        #print (lstmOut_lastTimeStep.size())\n",
        "        \n",
        "        #lstmOut = lstmOut.view(L*B,D)\n",
        "        outputLayerActivations = self.outputLayer(lstmOut_lastTimeStep)\n",
        "        #outputSoftMax=self.softmax(outputLayerActivations)\n",
        "        # project lstm states to \"output\"\n",
        "        \n",
        "    \n",
        "        return outputLayerActivations.unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-YcmzrJa4BE"
      },
      "source": [
        "### Training the Network ###\n",
        "\n",
        "Please see the beginning of the below code snippet, to change $k$ and seqLs of training samples<br>\n",
        "\n",
        "The model is trained on sequencess of lengths in range of `minSeqLength` and `maxSeqLength`\n",
        "\n",
        "For simplicity, training code runs for a fixed number of epochs (or iterations). In practice, the training should be monitored with performance on a held-out or validation set, in order to avoid over-fitting.\n",
        "\n",
        "In the below training code the training runs for `min_epochs` and in each epoch 500 samples (== `iterations`) are fed. And remember that the loss is calculated for each sample and then backprop() is called. This is in contrast to batch learning where weights are updated after feeding in a bacth of samples. In our case there is no batching or batchSize =1\n",
        "\n",
        "We use the [`Adam` optimizer](https://arxiv.org/abs/1412.6980).\n",
        "\n",
        "The model runs fast enough to train on the CPU itself (GPUs are not used).\n",
        "\n",
        "Lets train our first RNN model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juQv09FQa4BH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "outputId": "ac06be6c-37e3-4ea7-ef37-03cda8602972"
      },
      "source": [
        "# set here the size of the RNN state:\n",
        "stateSize = 20\n",
        "# set here the size of the binary strings to be used for training:\n",
        "k=2 # we want the RNN to remember the number at 2nd position\n",
        "minSeqLength = 3\n",
        "maxSeqLength = 8\n",
        "\n",
        "## sequenceLengths would be in range in range minSeqLength - maxSeqLength\n",
        "\n",
        "\n",
        "# create the model:\n",
        "model = Memorize(stateSize)\n",
        "print ('Model initialized')\n",
        "\n",
        "# create the loss-function:\n",
        "lossFunction = nn.CrossEntropyLoss() # or nn.CrossEntropyLoss() -- see question #2 below\n",
        "\n",
        "# uncomment below to change the optimizers:\n",
        "#optimizer = optim.SGD(model.parameters(), lr=3e-2, momentum=0.8)\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
        "iterations = 500\n",
        "min_epochs = 20\n",
        "num_epochs,totalLoss = 0,float(\"inf\")\n",
        "lossList = []\n",
        "while num_epochs < min_epochs:\n",
        "    print(\"[epoch %d/%d] Avg. Loss for last 500 samples = %lf\"%(num_epochs+1,min_epochs,totalLoss))\n",
        "    num_epochs += 1\n",
        "    totalLoss = 0\n",
        "    for i in range(0,iterations):\n",
        "        # get a new random training sample:\n",
        "        sequenceLength = random.randint(minSeqLength,maxSeqLength)\n",
        "        x,y = getSample(sequenceLength,k)\n",
        " \n",
        "        model.zero_grad()\n",
        "\n",
        "        pred = model(x)\n",
        "\n",
        "        # compute the loss:\n",
        "        loss = lossFunction(pred,y)\n",
        "        totalLoss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        # perform the backward pass:\n",
        "        loss.backward()\n",
        "        # update the weights:\n",
        "        optimizer.step()\n",
        "    totalLoss=totalLoss/iterations\n",
        "    lossList.append(int(totalLoss))\n",
        "print('Training finished!')\n",
        "epochs =  np.arange(1,21)\n",
        "# plot the loss over epcohs:\n",
        "plt.plot(epochs,lossList)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.xticks(epochs,epochs)\n",
        "# plt.ylim([0,5]); \n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model initialized\n",
            "[epoch 1/20] Avg. Loss for last 500 samples = inf\n",
            "[epoch 2/20] Avg. Loss for last 500 samples = 2.240064\n",
            "[epoch 3/20] Avg. Loss for last 500 samples = 2.105440\n",
            "[epoch 4/20] Avg. Loss for last 500 samples = 1.967376\n",
            "[epoch 5/20] Avg. Loss for last 500 samples = 0.542984\n",
            "[epoch 6/20] Avg. Loss for last 500 samples = 0.021714\n",
            "[epoch 7/20] Avg. Loss for last 500 samples = 0.005379\n",
            "[epoch 8/20] Avg. Loss for last 500 samples = 0.002598\n",
            "[epoch 9/20] Avg. Loss for last 500 samples = 0.001682\n",
            "[epoch 10/20] Avg. Loss for last 500 samples = 0.001092\n",
            "[epoch 11/20] Avg. Loss for last 500 samples = 0.000738\n",
            "[epoch 12/20] Avg. Loss for last 500 samples = 0.000551\n",
            "[epoch 13/20] Avg. Loss for last 500 samples = 0.000426\n",
            "[epoch 14/20] Avg. Loss for last 500 samples = 0.028112\n",
            "[epoch 15/20] Avg. Loss for last 500 samples = 0.046421\n",
            "[epoch 16/20] Avg. Loss for last 500 samples = 0.001473\n",
            "[epoch 17/20] Avg. Loss for last 500 samples = 0.000837\n",
            "[epoch 18/20] Avg. Loss for last 500 samples = 0.000475\n",
            "[epoch 19/20] Avg. Loss for last 500 samples = 0.000399\n",
            "[epoch 20/20] Avg. Loss for last 500 samples = 0.000254\n",
            "Training finished!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfXRcd33n8fdHkh81SuIHjQi2YzselSUUkoAaaEkhFAgO202ghTZpoaHA+rRLWqBddpPSk9DQPQdKW3paUlKXeHloSKCBgGkDSXhMuxCIHJxngmWT1HYTS7EdP8e2pO/+ca+SiTKSZsZzdWekz+ucOZq59/7u/coezUf33p9+P0UEZmZm47XlXYCZmTUnB4SZmVXkgDAzs4ocEGZmVpEDwszMKurIu4BGWrp0aaxatSrvMszMWsamTZueiIjuSutmVECsWrWK/v7+vMswM2sZkh6daJ0vMZmZWUUOCDMzq8gBYWZmFTkgzMysIgeEmZlVlFlASFoh6TuSHpT0gKT3VthGkv5W0oCkeyW9tGzdpZK2pI9Ls6rTzMwqy7Kb6zDwxxFxt6QuYJOk2yPiwbJtLgB608fLgU8CL5e0GLgK6AMibbsxIvZmWK+ZmZXJLCAi4jHgsfT5AUkPAcuA8oC4CPhsJGOO3ynpFEmnAucBt0fEHgBJtwNrgRuyqPVvv7WF4ZHR+ncg8eazl7F6aWfjijIzy9m0/KGcpFXA2cAPx61aBmwve70jXTbR8kr7XgesAzjttNPqqu/a723lyPGRutoCRMCufU/x0be8pO59mJk1m8wDQlIB+BLwvojY3+j9R8R6YD1AX19fXbMfPXj12hOq4eL1P2DL4IET2oeZWbPJtBeTpDkk4XB9RHy5wiY7gRVlr5enyyZa3pR6i10MDB7Es/OZ2UySZS8mAdcBD0XEX0+w2Ubgd9LeTK8A9qX3Lm4Fzpe0SNIi4Px0WVMqFQvsf2qYoQNH8y7FzKxhsrzE9Erg7cB9kjany/4EOA0gIq4FbgHeCAwAh4HfTdftkfRh4K603dVjN6ybUW+xAMDA4EGKJ83PuRozs8bIshfTvwOaYpsA3jPBug3AhgxKa7hSGhBbBg/yS6WlOVdjZtYY/kvqBujumsdJ8zsYGDyYdylmZg3jgGgASZSKBfdkMrMZxQHRIElPpkN5l2Fm1jAOiAYpFQs8cfAoTx4+lncpZmYN4YBokFLPMz2ZzMxmAgdEg5S6n+nJZGY2EzggGmTZKQtYMKfdZxBmNmM4IBqkrU2sKXb6DMLMZgwHRAP1FrvY6oAwsxnCAdFApWKBnU8e4dDR4bxLMTM7YQ6IBhobcmPrkM8izKz1OSAa6OkxmXY5IMys9TkgGmjl4oXMaRcDPoMwsxnAAdFAHe1trF7a6TMIM5sRHBANVioWfA/CzGYEB0SDlYpdPLr7EE8dH8m7FDOzE5LllKMbJA1Kun+C9R+QtDl93C9pRNLidN0jku5L1/VnVWMWSsUCowGP7PbIrmbW2rI8g/g0sHailRHxsYg4KyLOAq4AvjduWtHXpOv7Mqyx4Xrdk8nMZojMAiIi7gCqnUf6EuCGrGqZTquXdtImj+pqZq0v93sQkhaSnGl8qWxxALdJ2iRp3RTt10nql9Q/NDSUZalVmT+nndMWL3RAmFnLyz0ggP8G/L9xl5fOjYiXAhcA75H0qokaR8T6iOiLiL7u7u6sa61KqVhwQJhZy2uGgLiYcZeXImJn+nUQuBk4J4e66lYqdrHtiYMMj4zmXYqZWd1yDQhJJwOvBr5atqxTUtfYc+B8oGJPqGZVKhY4PhL8x57DeZdiZla3jqx2LOkG4DxgqaQdwFXAHICIuDbd7M3AbRFR3ie0B7hZ0lh9n4+Ib2RVZxae7sk0eJDT05nmzMxaTWYBERGXVLHNp0m6w5Yv2wacmU1V02NN8Zn5qd/wopyLMTOrUzPcg5hxCvM6eP7J832j2sxamgMiI2vck8nMWpwDIiO9xS4GBg8yOhp5l2JmVhcHREZKxQJHjo/wn/uO5F2KmVldHBAZ6e15pieTmVkrckBkpJR2b93qgDCzFuWAyMiizrksLcz1qK5m1rIcEBla013w/NRm1rIcEBnq7SmwZdcBItyTycxajwMiQ6XuAvufGmbo4NG8SzEzq5kDIkO9PV0ADPg+hJm1IAdEhkpjYzL5PoSZtSAHRIaKXfPomt/hnkxm1pIcEBmS5NnlzKxlOSAy1lt0V1cza00OiIyVigWGDhxl3+HjeZdiZlaTzAJC0gZJg5IqThcq6TxJ+yRtTh9Xlq1bK+lhSQOSLs+qxunQW0x7Mg0dyLkSM7PaZHkG8Wlg7RTb/FtEnJU+rgaQ1A5cA1wAnAFcIumMDOvM1FhPJt+oNrNWk1lARMQdwJ46mp4DDETEtog4BtwIXNTQ4qbRslMWMH9Om29Um1nLyfsexC9KukfS1yWNzd68DNhets2OdFlFktZJ6pfUPzQ0lGWtdWlrE2u6Cx7228xaTp4BcTewMiLOBP4O+Eo9O4mI9RHRFxF93d3dDS2wUdzV1cxaUW4BERH7I+Jg+vwWYI6kpcBOYEXZpsvTZS2rt1hg55NHOHR0OO9SzMyqlltASHqeJKXPz0lr2Q3cBfRKWi1pLnAxsDGvOhth7Eb1tqFDOVdiZla9jqx2LOkG4DxgqaQdwFXAHICIuBZ4C/D7koaBI8DFkYyLPSzpMuBWoB3YEBEPZFXndCilXV23DB7gxctPzrkaM7PqZBYQEXHJFOs/AXxignW3ALdkUVceVi5ZSEebfB/CzFpK3r2YZoU57W2sXtrpnkxm1lIcENOkVCyw1QFhZi3EATFNeosFHtl9iKPDI3mXYmZWFQfENFlTLDAa8MgTh/MuxcysKg6IadJb1pPJzKwVOCCmyendnUi4J5OZtQwHxDSZP6ed0xYvdE8mM2sZDohpVOp2TyYzax0OiGlU6imwbegQwyOjeZdiZjYlB8Q0KnUXODYyyva9R/IuxcxsSg6IadTbk/Zk2uWeTGbW/BwQ02hNdycAA0O+D2Fmzc8BMY265s/h1JPnM+D5qc2sBTggplmpWPAZhJm1BAfENBubfnR0NPIuxcxsUpkFhKQNkgYl3T/B+t+WdK+k+yR9X9KZZeseSZdvltSfVY15KBULHD42wmP7n8q7FDOzSWV5BvFpYO0k638GvDoiXgx8GFg/bv1rIuKsiOjLqL5cPD0mk3symVmTyywgIuIOYM8k678fEXvTl3cCy7OqpZmMzU/tMZnMrNk1yz2IdwFfL3sdwG2SNklaN1lDSesk9UvqHxoayrTIRljcOZclnXMdEGbW9DKbk7pakl5DEhDnli0+NyJ2SioCt0v6SXpG8hwRsZ708lRfX19L3Pldk96oNjNrZrmeQUh6CfAp4KKI2D22PCJ2pl8HgZuBc/KpMBu9xQJbBg8S0RJ5ZmazVG4BIek04MvA2yPip2XLOyV1jT0Hzgcq9oRqVaVigX1HjvPEwWN5l2JmNqHMLjFJugE4D1gqaQdwFTAHICKuBa4ElgB/LwlgOO2x1APcnC7rAD4fEd/Iqs48lM8u1901L+dqzMwqyywgIuKSKda/G3h3heXbgDOf22LmGOvJtHXwIL+0ZmnO1ZiZVdYsvZhmlZ6T5tE1r8Ozy5lZU3NA5ECSezKZWdNzQOSklPZkMjNrVg6InPQWCwwdOMq+w8fzLsXMrCIHRE6eHnJjyGMymVlzckDkZKyrq+9DmFmzckDkZNmiBczraGOLZ5czsyblgMhJe5tY0+3Z5cyseTkgclQqFnwGYWZNywGRo95igZ1PHuHwseG8SzEzew4HRI6eGXLjUM6VmJk9lwMiR7097upqZs3LAZGjlUs66WiT70OYWVOqKiAkvVfSSUpcJ+luSednXdxMN6e9jVVLO/23EGbWlKo9g3hnROwnmbxnEfB24COZVTWLlLo9aJ+ZNadqA0Lp1zcCn4uIB8qW2Qno7Snw6J7DHB0eybsUM7NnqTYgNkm6jSQgbk2nBB2dqpGkDZIGJVWcMjS9ZPW3kgYk3SvppWXrLpW0JX1cWmWdLadULDAyGjzyxOG8SzEze5ZqA+JdwOXAL0TEYZKpQ3+3inafBtZOsv4CoDd9rAM+CSBpMckUpS8HzgGukrSoylpbytOD9vkyk5k1mWoD4heBhyPiSUlvA/4U2DdVo4i4A9gzySYXAZ+NxJ3AKZJOBd4A3B4ReyJiL3A7kwdNy1rTXUBK5qc2M2sm1QbEJ4HDks4E/hjYCny2AcdfBmwve70jXTbR8ueQtE5Sv6T+oaGhBpQ0vebPaWfFooU+gzCzplNtQAxHRJD8xv+JiLgG6MqurOpFxPqI6IuIvu7u7rzLqUvJ04+aWROqNiAOSLqCpHvrv0pqI7kPcaJ2AivKXi9Pl020fEbqLRbY9sQhhkemvO9vZjZtqg2I3wSOkvw9xOMkH9gfa8DxNwK/k/ZmegWwLyIeA24Fzpe0KL05fX66bEZaUyxwbHiU7XuP5F2KmdnTOqrZKCIel3Q98AuSfhX4UURMeQ9C0g3AecBSSTtIeibNSfd5LXALSdfZAeAwac+oiNgj6cPAXemuro6IyW52t7Tesp5Mq5d25lyNmVmiqoCQ9BskZwzfJfkDub+T9IGIuGmydhFxyRTrA3jPBOs2ABuqqa/VrUkDYsvgAV5/Rk/O1ZiZJaoKCOCDJH8DMQggqRv4JjBpQFh1Tpo/h+edNN83qs2sqVR7D6JtLBxSu2toa1VwTyYzazbVfsh/Q9Ktkt4h6R3Av5LcP7AGGQuI5KqbmVn+qr1J/QFJvw68Ml20PiJuzq6s2adULHD42Aj/ue8plp2yIO9yzMyqvgdBRHwJ+FKGtcxq5T2ZHBBm1gwmvcQk6YCk/RUeByTtn64iZ4OxQfu27PKYTGbWHCY9g4iIphhOYzZYUpjH4s65bB3yjWozaw7uidRESt0Fz09tZk3DAdFESj0Ftrgnk5k1CQdEEyl1F9h35DhPHDyWdylmZg6IZtLb49nlzKx5OCCayDPTj7onk5nlzwHRRJ530nwK8zp8BmFmTcEB0UQksaaY3Kg2M8ubA6LJ9HrQPjNrEg6IJlMqFhg8cJR9R47nXYqZzXKZBoSktZIeljQg6fIK6z8uaXP6+KmkJ8vWjZSt25hlnc2kfEwmM7M8VT1YX60ktQPXAK8HdgB3SdoYEQ+ObRMR7y/b/g+As8t2cSQizsqqvmY11pNp6+BBXrZyUc7VmNlsluUZxDnAQERsi4hjwI3ARZNsfwlwQ4b1tITlixYyr6ONLe7qamY5yzIglgHby17vSJc9h6SVwGrg22WL50vql3SnpDdNdBBJ69Lt+oeGhhpRd67a28Tp3b5RbWb5a5ab1BcDN0XESNmylRHRB/wW8DeS1lRqGBHrI6IvIvq6u7uno9bM9bqrq5k1gSwDYiewouz18nRZJRcz7vJSROxMv24Dvsuz70/MaKVigZ1PHuHwseG8SzGzWSzLgLgL6JW0WtJckhB4Tm8kSf8FWAT8oGzZIknz0udLSaY6fXB825mqt1ggArYNHcq7FDObxTILiIgYBi4DbgUeAr4YEQ9IulrShWWbXgzcGM8e4/qFQL+ke4DvAB8p7/0005Xc1dXMmkBm3VwBIuIW4JZxy64c9/pDFdp9H3hxlrU1s5VLOulok3symVmumuUmtZWZ29HGyiULfQZhZrlyQDSp3mKXezKZWa4cEE2qVCzw6O7DHBsezbsUM5ulHBBNqlQsMDIaPLLbPZnMLB8OiCblnkxmljcHRJNa011Agi27HBBmlg8HRJNaMLed5YsWMDDkgDCzfDggmlipu8CWXf5bCDPLhwOiifX2dLHtiUOMjMbUG5uZNZgDoomVugscGx5l+57DeZdiZrOQA6KJlXrck8nM8uOAaGJjXV39F9VmlgcHRBM7af4cek6a5zMIM8uFA6LJlYoFBjyqq5nlwAHR5HqLXQwMHuTZ02WYmWUv04CQtFbSw5IGJF1eYf07JA1J2pw+3l227lJJW9LHpVnW2czWFAscOjbCY/ueyrsUM5tlMpswSFI7cA3wemAHcJekjRVmhvtCRFw2ru1i4CqgDwhgU9p2b1b1NqvesjGZnn/KgpyrMbPZJMsziHOAgYjYFhHHgBuBi6ps+wbg9ojYk4bC7cDajOpsau7JZGZ5yTIglgHby17vSJeN9+uS7pV0k6QVNbZF0jpJ/ZL6h4aGGlF3U1nSOZdFC+e4J5OZTbu8b1J/DVgVES8hOUv4TK07iIj1EdEXEX3d3d0NLzBvktyTycxykWVA7ARWlL1eni57WkTsjoij6ctPAS+rtu1sUkqnH3VPJjObTlkGxF1Ar6TVkuYCFwMbyzeQdGrZywuBh9LntwLnS1okaRFwfrpsVioVCzx5+Di7Dx3LuxQzm0Uy68UUEcOSLiP5YG8HNkTEA5KuBvojYiPwh5IuBIaBPcA70rZ7JH2YJGQAro6IPVnV2uzKezItLczLuRozmy0yCwiAiLgFuGXcsivLnl8BXDFB2w3AhizraxXlPZlecfqSnKsxs9ki75vUVoVTT55P59x2tronk5lNIwdECxjrybTFPZnMbBo5IFpEKR2TycxsujggWkSpWGDX/qPsf+p43qWY2SzhgGgR5T2ZzMymgwOiRYz1ZBrY5YAws+nhgGgRKxYvZG5HGwNDDggzmx4OiBbR3iZOX9rJll3uyWRm08MB0UJ6e7p8BmFm08YB0UJK3QV27D3CkWMjeZdiZrOAA6KF9PYUiICtPosws2nggGghJXd1NbNp5IBoIauWdNLeJgeEmU0LB0QLmdvRxsolCz0mk5lNCwdEi+ktFnwGYWbTwgHRYkrFAo/sPsyx4dG8SzGzGS7TgJC0VtLDkgYkXV5h/R9JelDSvZK+JWll2boRSZvTx8bxbWer3mIXI6PBo7sP5V2Kmc1wmQWEpHbgGuAC4AzgEklnjNvsx0BfRLwEuAn4i7J1RyLirPRxYVZ1tpry2eXMzLKU5RnEOcBARGyLiGPAjcBF5RtExHci4nD68k5geYb1zAhrugtI7upqZtnLMiCWAdvLXu9Il03kXcDXy17Pl9Qv6U5Jb5qokaR16Xb9Q0NDJ1ZxC1gwt51lpyzwGYSZZa4j7wIAJL0N6ANeXbZ4ZUTslHQ68G1J90XE1vFtI2I9sB6gr68vpqXgnLknk5lNhyzPIHYCK8peL0+XPYuk1wEfBC6MiKNjyyNiZ/p1G/Bd4OwMa20ppWKBrUMHGRmdFXloZjnJMiDuAnolrZY0F7gYeFZvJElnA/9AEg6DZcsXSZqXPl8KvBJ4MMNaW0pvsYtjw6Ps2Ht46o3NzOqUWUBExDBwGXAr8BDwxYh4QNLVksZ6JX0MKAD/PK476wuBfkn3AN8BPhIRDojUmrGeTJ5dzswylOk9iIi4Bbhl3LIry56/boJ23wdenGVtrezpQfuGDvI6enKuxsxmKv8ldQs6ecEcil3zfAZhZplyQLSo3p6CZ5czs0w5IFpUqbvA1sGDRLgnk5llwwHRoko9XRw8Oszj+5/KuxQzm6EcEC2q1O2eTGaWLQdEi+rt8fSjZpYtB0SLWtI5l1MWzvGYTGaWGQdEi5JEbzG5UW1mlgUHRAsrFQuen9rMMuOAaGGlYhd7Dx9n98GjU29sZlYjB0QL8+xyZpYlB0QL6y26J5OZZccB0cJOPXk+nXPbHRBmlgkHRAuTRMmzy5lZRhwQLW6NezKZWUYcEC2ut9jFrv1H2f/U8bxLMbMZJtOAkLRW0sOSBiRdXmH9PElfSNf/UNKqsnVXpMsflvSGLOtsZSXfqDazjGQWEJLagWuAC4AzgEsknTFus3cBeyOiBHwc+Gja9gySOaxfBKwF/j7dn43jnkxmlpUspxw9BxiIiG0Akm4ELgLK55a+CPhQ+vwm4BOSlC6/MSKOAj+TNJDu7wcZ1tuSVixeyNyONj769Z/wj3dsy7scM8vBooVz+eLv/WLD95tlQCwDtpe93gG8fKJtImJY0j5gSbr8znFtl1U6iKR1wDqA0047rSGFt5L2NvGB81/Aj7fvzbsUM8vJSfPnZLLfLANiWkTEemA9QF9f36ycXu2/v+r0vEswsxkoy5vUO4EVZa+Xp8sqbiOpAzgZ2F1lWzMzy1CWAXEX0CtptaS5JDedN47bZiNwafr8LcC3I5lkeSNwcdrLaTXQC/wow1rNzGyczC4xpfcULgNuBdqBDRHxgKSrgf6I2AhcB3wuvQm9hyRESLf7IskN7WHgPRExklWtZmb2XEp+YZ8Z+vr6or+/P+8yzMxahqRNEdFXaZ3/ktrMzCpyQJiZWUUOCDMzq8gBYWZmFc2om9SShoBH62y+FHjiBA7v9m7v9m7fiu1XRkR3xTUR4UcSkv1u7/Zu7/azsf1ED19iMjOzihwQZmZWkQPiGevd3u3d3u1nafuKZtRNajMzaxyfQZiZWUUOCDMzq2jWB4SkDZIGJd1fR9sVkr4j6UFJD0h6b43t50v6kaR70vZ/VmsN6X7aJf1Y0r/U0fYRSfdJ2iyp5pEOJZ0i6SZJP5H0kKSa5j2U9IL02GOP/ZLeV0P796f/dvdLukHS/BqP/9607QPVHrfSe0bSYkm3S9qSfl1UY/u3pjWMSqo4cNoU7T+W/h/cK+lmSafU2P7DadvNkm6T9Pxa2pet+2NJIWlpjcf/kKSdZe+DN9Z6fEl/kP4bPCDpL2o8/hfKjv2IpM01tj9L0p1jP0eSzqmx/ZmSfpD+LH5N0kmTtK/4uVPLe7BqWfSdbaUH8CrgpcD9dbQ9FXhp+rwL+ClwRg3tBRTS53OAHwKvqKOOPwI+D/xLHW0fAZaewL/fZ4B3p8/nAqecwL7agcdJ/nCnmu2XAT8DFqSvvwi8o4bj/TxwP7CQZOj7bwKlet4zwF8Al6fPLwc+WmP7FwIvAL4L9NVx/POBjvT5R+s4/kllz/8QuLaW9unyFSTD+z862XtqguN/CPifVf6/VWr/mvT/b176ulhr/WXr/wq4ssbj3wZckD5/I/DdGtvfBbw6ff5O4MOTtK/4uVPLe7Dax6w/g4iIO0jmoqin7WMRcXf6/ADwEBPMnT1B+4iIg+nLOemjpl4DkpYD/xX4VC3tGkHSySRv9usAIuJYRDx5Art8LbA1Imr5a/gOYIGSGQkXAv9ZQ9sXAj+MiMMRMQx8D/i1qRpN8J65iCQsSb++qZb2EfFQRDxcTdETtL8t/R4gmc99eY3t95e97GSS9+EkPzMfB/7XZG2naF+VCdr/PvCRiDiabjNYz/ElCfgN4IYa2wcw9lv/yUzyPpyg/c8Bd6TPbwd+fZL2E33uVP0erNasD4hGkbQKOJvkLKCWdu3p6ewgcHtE1NQe+BuSH8rRGtuNCeA2SZskraux7WpgCPi/6SWuT0nqrLMOSCaMmvAHc7yI2An8JfAfwGPAvoi4rYbj3Q/8sqQlkhaS/Oa3Yoo2E+mJiMfS548DPXXupxHeCXy91kaS/o+k7cBvA1fW2PYiYGdE3FPrcctcll7m2lDH5ZGfI/m//KGk70n6hTpr+GVgV0RsqbHd+4CPpf9+fwlcUWP7B0g+4AHeSpXvw3GfOw1/DzogGkBSAfgS8L5xv4lNKSJGIuIskt/4zpH08zUc91eBwYjYVFPBz3ZuRLwUuAB4j6RX1dC2g+RU+ZMRcTZwiOTUtmZKpqW9EPjnGtosIvmhWg08H+iU9LZq20fEQySXY24DvgFsBk545sJIzvFz6T8u6YMkszBeX2vbiPhgRKxI215WwzEXAn9CjaEyzieBNcBZJGH/VzW27wAWA68APgB8MT0bqNUl1PBLSpnfB96f/vu9n/SsugbvBP6HpE0kl42OTdVgss+dRr0HHRAnSNIckv+k6yPiy/XuJ7008x1gbQ3NXglcKOkR4EbgVyT9U43H3Zl+HQRuBia8uVbBDmBH2VnPTSSBUY8LgLsjYlcNbV4H/CwihiLiOPBl4JdqOWhEXBcRL4uIVwF7Sa7n1mOXpFMB0q8TXuLIiqR3AL8K/Hb6AVGv65nkEkcFa0hC+p70vbgcuFvS86rdQUTsSn9ZGgX+kdreh5C8F7+cXrb9EckZ9YQ3yitJL1P+GvCFGo8NcCnJ+w+SX3Jqqj8ifhIR50fEy0gCausUtVb63Gn4e9ABcQLS31CuAx6KiL+uo333WG8TSQuA1wM/qbZ9RFwREcsjYhXJ5ZlvR0TVv0FL6pTUNfac5EZn1b25IuJxYLukF6SLXksyj3g96vnN7T+AV0hamP5fvJbkemzVJBXTr6eRfDh8vsYaxmwk+ZAg/frVOvdTF0lrSS41XhgRh+to31v28iJqex/eFxHFiFiVvhd3kNxEfbyG459a9vLN1PA+TH2F5EY1kn6OpMNEraObvg74SUTsqLEdJPccXp0+/xWgpktUZe/DNuBPgWsn2Xaiz53GvwdP9C53qz9IPpQeA46TvLHfVUPbc0lO4+4luTyxGXhjDe1fAvw4bX8/k/ScqGJf51FjLybgdOCe9PEA8ME6jnsW0J9+D18BFtWxj05gN3ByHW3/jOTD7H7gc6S9WGpo/28koXYP8Np63zPAEuBbJB8M3wQW19j+zenzo8Au4NYa2w8A28veh5P1QqrU/kvpv+G9wNeAZfX+zDBFz7gJjv854L70+BuBU2tsPxf4p/R7uBv4lVrrBz4N/F6d///nApvS99EPgZfV2P69JGevPwU+QjrKxQTtK37u1PIerPbhoTbMzKwiX2IyM7OKHBBmZlaRA8LMzCpyQJiZWUUOCDMzq8gBYZYjSeepjlF4zaaDA8LMzCpyQJhVQdLblMzdsVnSP6SDLB6U9PF0TP5vSepOtx2bG2BsboZF6fKSpG8qmf/jbklr0t0X9MycGtePjSEk6SPpmP/3SvrLnL51m8UcEGZTkPRC4DeBV0YysOIIyYinnUB/RLyIZKjwq9ImnwX+d0S8hOSvg8eWXw9cExFnkowZNTby5tkko4GeQfLX7a+UtITkr6tflO7nz7P9Ls2eywFhNrXXAi8D7kqHZn8tyQf5KM8M7PZPwLnpHBmnRMT30uWfAV6Vjnm1LCJuBoiIp+KZMZN+FBE7IhmobjOwCoL8avoAAAEESURBVNgHPAVcJ+nXgJrHVzI7UQ4Is6kJ+ExEnJU+XhARH6qwXb3j1hwtez5CMjPcMMmIoDeRjND6jTr3bVY3B4TZ1L4FvKVsxM3FklaS/Py8Jd3mt4B/j4h9wF5Jv5wufzvwvUhm/toh6U3pPual8yhUlI71f3JE3EIyv8CZWXxjZpPpyLsAs2YXEQ9K+lOSmffaSEbhfA/JBEnnpOsGSe5TQDLU8rVpAGwDfjdd/nbgHyRdne7jrZMctgv4qqT5JGcwf9Tgb8tsSh7N1axOkg5GRCHvOsyy4ktMZmZWkc8gzMysIp9BmJlZRQ4IMzOryAFhZmYVOSDMzKwiB4SZmVX0/wE0PZ9uasDfLQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMBu0K8qa4BN"
      },
      "source": [
        "## Testing the Model ###\n",
        "Lets feed in a sample sequence and see if the network can output the number at the $k^{th}$ position"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Yj5A-db3a4BP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffa0dad1-7911-4549-e9ff-049a77e94f63"
      },
      "source": [
        "testSeqL = 6\n",
        "x,y = getSample(testSeqL,k,testFlag=True)\n",
        "\n",
        "pred = model(x)\n",
        "ind=  torch.argmax(pred)\n",
        "print ( 'number at kth position is ',int(ind))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 3 2 9 2 5 --> 3\n",
            "number at kth position is  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHSOfKyMa4BQ"
      },
      "source": [
        "__Question 1:__  \n",
        "\n",
        "Above testing code snippet is  only for one sample. Test 500 samples of random seqLens in [3,7] range and compute the average accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3cMYboDa4BS"
      },
      "source": [
        "__Question 2:__  Use vanilla RNN instead of the LSTM in the model and do the following\n",
        "  1. plot the epochs vs loss like in the above case when vanilla RNN is used (without changing `k`, `stateSize`, `minSeqL` and `maxSeqL`)\n",
        "  2. Make  `minSeqL` and `maxSeqL` = 10 and  `k`=9 and repeat the experiment with vanilla RNN\n",
        "  3. Make  `minSeqL` and `maxSeqL` = 3 and  `k`=2 and repeat the experiment with vanilla RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be2eVx5Da4BU"
      },
      "source": [
        "__Question 3:__ \n",
        " Train  on fixed sequence lengths and see if it works for shorter and longer sequences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mLhj5ZMa4BV"
      },
      "source": [
        "__Question 4:__  Try MSE ( Mean Square Error) loss instead of CrossEntropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FknB0bY6a4BV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O9RW4DZa4BW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}