{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "3_Weight_Initialization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThrupthiAnn/SummerSchool2021_HandsOn_Aug7/blob/main/CNN/3_Weight_Initialization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pME8T11-JnCm"
      },
      "source": [
        "## Trying out different weight initializer techniques\n",
        "\n",
        "We begin with building a CNN architecture for image classification task on CIFAR10 dataset. In this part of the tutorial, we will understand how to use  different weight initializer techniques to train a CNN network.\n",
        "\n",
        "To make data loading simple, we would use the torchvision package created as part of PyTorch which has data loaders for standard datasets such as ImageNet, CIFAR10, MNIST.\n",
        "\n",
        "### CIFAR10 dataset\n",
        "![CIFAR10](https://github.com/ckraju/summer/blob/main/data/resnet/cifar10.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "WZXOfXeZJnCp"
      },
      "source": [
        "#a Tensor library with GPU support\n",
        "import torch\n",
        "\n",
        "#Datasets, Transforms and Models specific to Computer Vision\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "#differentiation library that supports all differentiable Tensor operations in torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "#a neural networks library integrated with autograd functionality\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#an optimization package with standard optimization methods such as SGD, RMSProp, LBFGS, Adam etc.\n",
        "import torch.optim as optim\n",
        "\n",
        "#Weight Initialization\n",
        "import torch.nn.init as weight_init\n",
        "\n",
        "#scientific computing library for Python\n",
        "import numpy as np\n",
        "\n",
        "#plotting and visualization library\n",
        "import matplotlib.pyplot as plt\n",
        "#Display on the notebook\n",
        "%matplotlib inline \n",
        "plt.ion() #Turn interactive mode on."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xdDnxkKJnCr"
      },
      "source": [
        "### Dataloader and Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ykUm1zgWJnCs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74141ebf-215a-4c55-9178-d8fb4318f748"
      },
      "source": [
        "#Train data\n",
        "#Compose transforms (applies data transformation and augmentation) prior to feeding to training\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "#inbuilt dataset class for reading CIFAR10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='../../data/lab1', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "#dataloader for Batching, shuffling and loading data in parallel\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "#test data\n",
        "testset = torchvision.datasets.CIFAR10(root='../../data/lab1', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myXDr1poJnCu"
      },
      "source": [
        "\n",
        "### Defining the model\n",
        "\n",
        "To create a network, we should first inherit the base class nn.Module. You just have to define the forward function, and the backward function (where gradients are computed) is automatically defined for you using autograd. You can use any of the Tensor operations in the forward function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4HGdvpaJnCv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfea85d9-5d7c-4632-bc51-999a6afaccdd"
      },
      "source": [
        "\n",
        "# This scheme can be one of 'uniform', 'normal', 'constant' 'Xavier' and 'custom'\n",
        "\n",
        "weight_initialization_scheme = 'uniform'\n",
        "\n",
        "\n",
        "def weight_init_custom_conv(module):\n",
        "    import math\n",
        "    n = module.kernel_size[0] * module.kernel_size[1] * module.out_channels\n",
        "    module.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "\n",
        "def weight_init_custom_linear(module):\n",
        "    import math\n",
        "#     import pdb\n",
        "#     pdb.set_trace()\n",
        "    n = module.in_features * module.out_features\n",
        "    module.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "\n",
        "    \n",
        "class Model(nn.Module):\n",
        "    \n",
        "    #define the learnable paramters by calling the respective modules (nn.Conv2d, nn.MaxPool2d etc.)\n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "        #calling conv2d module for convolution\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5,stride=1,padding=0,bias=True)\n",
        "        \n",
        "        #calling MaxPool2d module for max pooling with downsampling of 2\n",
        "        self.pool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.pool_2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        \n",
        "        #fully connected layers\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)   \n",
        "        \n",
        "        \n",
        "        # Setting the weights for the conv1 layer\n",
        "        for m in self.modules():\n",
        "            if weight_initialization_scheme == 'uniform':\n",
        "#                     print('Initializating with uniform scheme')\n",
        "                    weight_init.uniform(self.conv1.weight)\n",
        "                    weight_init.uniform(self.conv2.weight)\n",
        "                    weight_init.uniform(self.fc1.weight)\n",
        "                    weight_init.uniform(self.fc2.weight)\n",
        "                    weight_init.uniform(self.fc3.weight)\n",
        "            if weight_initialization_scheme == 'normal':\n",
        "#                     print('Initializating with normal scheme')\n",
        "                    weight_init.normal(self.conv1.weight)\n",
        "                    weight_init.normal(self.conv2.weight)\n",
        "                    weight_init.normal(self.fc1.weight)\n",
        "                    weight_init.normal(self.fc2.weight)\n",
        "                    weight_init.normal(self.fc3.weight)\n",
        "            if weight_initialization_scheme == 'constant':\n",
        "#                     print('Initializating with constant scheme')\n",
        "                    weight_init.constant(self.conv1.weight, 0.1)\n",
        "                    weight_init.constant(self.conv2.weight, 0.1)\n",
        "                    weight_init.constant(self.fc1.weight, 0.1)\n",
        "                    weight_init.constant(self.fc2.weight, 0.1)\n",
        "                    weight_init.constant(self.fc3.weight, 0.1)\n",
        "            if weight_initialization_scheme == 'Xavier':\n",
        "#                     print('Initializating with Xavier scheme')\n",
        "                    weight_init.xavier_normal(self.conv1.weight)\n",
        "                    weight_init.xavier_normal(self.conv2.weight)\n",
        "                    weight_init.xavier_normal(self.fc1.weight)\n",
        "                    weight_init.xavier_normal(self.fc2.weight)\n",
        "                    weight_init.xavier_normal(self.fc3.weight)\n",
        "            if weight_initialization_scheme == 'custom':\n",
        "#                     print('Initializating with custom scheme')\n",
        "                    weight_init_custom_conv(self.conv1)\n",
        "                    weight_init_custom_conv(self.conv2)\n",
        "                    weight_init_custom_linear(self.fc1)\n",
        "                    weight_init_custom_linear(self.fc2)\n",
        "                    weight_init_custom_linear(self.fc3)\n",
        "                    \n",
        "\n",
        "\n",
        "    \n",
        "    #defining the structure of the network\n",
        "    def forward(self, x):\n",
        "        \n",
        "        #Applying relu activation after each conv layer\n",
        "        x = self.pool_1(F.relu(self.conv1(x)))\n",
        "        x = self.pool_2(F.relu(self.conv2(x)))\n",
        "        \n",
        "        #reshaping to 1d for giving input to fully connected units\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = Model()\n",
        "model = model.cuda()\n",
        "\n",
        "#Printing the network architecture\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:47: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MqUZxTVJnCx"
      },
      "source": [
        "### Define the optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "7c61-nnwJnCy"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fphhmhrJnCz"
      },
      "source": [
        "### Train the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0776ch4JnC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96e125a-e05b-4479-8e4b-1d57f5ee6106"
      },
      "source": [
        "for epoch in range(5):  # loop over the dataset multiple times\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    for i, data in enumerate(train_loader):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # wrap them in Variable\n",
        "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        #print (loss.data.item() - loss.data.cpu())\n",
        "        #print (loss.data.cpu())\n",
        "        #print ((loss.data[0]).item())\n",
        "        total_loss += loss.data.cpu()\n",
        "        # Calculate no of correct classifications\n",
        "        _, predicted_class = outputs.max(1)\n",
        "        correct += predicted_class.data.eq(labels.data).sum()     \n",
        "        \n",
        "    print(\"Epoch: {0} | loss: {1} | accuracy: {2}\".format(epoch, total_loss/len(train_loader)\n",
        "                                                          , correct/float(len(train_loader.dataset))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 | loss: 2.3082973957061768 | accuracy: 0.0995199978351593\n",
            "Epoch: 1 | loss: 2.308103084564209 | accuracy: 0.10223999619483948\n",
            "Epoch: 2 | loss: 2.30814266204834 | accuracy: 0.09933999925851822\n",
            "Epoch: 3 | loss: 2.308627128601074 | accuracy: 0.09737999737262726\n",
            "Epoch: 4 | loss: 2.3084802627563477 | accuracy: 0.09781999886035919\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}